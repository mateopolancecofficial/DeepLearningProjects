{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SemanticSegmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/Q55MEcY+4fsHCdXN42FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/DeepLearningProjects/blob/main/Regression/Vineyard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gaL1fd3f1d2"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9EZPU4_P4S1",
        "outputId": "e35c878c-cd78-4f2a-9849-120967bd54fc"
      },
      "source": [
        "pip install -q -U keras-tuner\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20kB 22.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30kB 27.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40kB 29.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 5.0MB/s \n",
            "\u001b[?25h  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_qp9YS8gKiM"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Input\n",
        ")\n",
        "\n",
        "import kerastuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soni8IcxQbIZ"
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwv8sDiTPv_5"
      },
      "source": [
        "# set column names\n",
        "columns = ['x0', 'x1', 'x2', 'x3', 'u']\n",
        "# define ranges of all features\n",
        "feature_ranges = {\"x0\": [1, 17], \"x1\": [-9, -4], \"x2\": [0, 3], \"x3\": [0, 50]}\n",
        "# path to data source\n",
        "source_path = \"./Data/podaci.csv\"\n",
        "# split sizes for train, validation and test subsets\n",
        "train_size, test_size, val_size = 0.8, 0.2, 0.2\n",
        "# num of new samples\n",
        "f_range = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6v4EYoLb56t"
      },
      "source": [
        "df = pd.read_csv(source_path, names=columns)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIkFVebcOhH"
      },
      "source": [
        "### Prepare datasets\n",
        "All relevant functions are tested in exploration notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbrlqq9sRZe1"
      },
      "source": [
        "def split_data(df: pd.DataFrame, features: list, target: list, train_size: float, \n",
        "               test_size: float, val_size: float):\n",
        "  \"\"\"\n",
        "  Split dataset on train, test and validation subsets.\n",
        "  :param df:          input pandas DataFrame\n",
        "  :param features:    list of input features names\n",
        "  :param target:      list of target column names\n",
        "  :param train_size:  fraction of train size\n",
        "  :param test_size:   fraction of test size\n",
        "  :param val_size:    fraction of validation size\n",
        "  :return:            dictionary, keys=names of DataFrame, columns=DataFrame\n",
        "  \"\"\"\n",
        "  \n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac = 1)\n",
        "  \n",
        "  # split on test and train set\n",
        "  x_train, x_test, y_train, y_test = train_test_split(df[features], df[target],\n",
        "                                     test_size=test_size, train_size=train_size)\n",
        "    \n",
        "  # split train set on train and validation subsets\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                      test_size=val_size, train_size=train_size)\n",
        "  \n",
        "  dataset_dict = {\n",
        "      'x_train': x_train,\n",
        "      'y_train': y_train,\n",
        "      'x_val': x_val,\n",
        "      'y_val': y_val,\n",
        "      'x_test': x_test,\n",
        "      'y_test': y_test\n",
        "  }\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuvOKPVRcAeK"
      },
      "source": [
        "def cause_relevant_features(df: pd.DataFrame, features: list, f_range: int):\n",
        "  \"\"\"\n",
        "  Cause new data based on calculated statistics.\n",
        "  :param df:          input pandas DataFrame\n",
        "  :param f_range:     int, num of new samples\n",
        "  :return new_df:     pandas DataFrame with new data\n",
        "  \"\"\"\n",
        "\n",
        "  new_data = {}\n",
        "\n",
        "  for idx in range(len(features)):\n",
        "\n",
        "    std = df[features[idx]].std()\n",
        "    mean = df[features[idx]].mean()\n",
        "\n",
        "    data_range = [mean - (1.5 * std), mean + (1.5 * std)]\n",
        "\n",
        "    new_data[features[idx]] = np.random.uniform(data_range[0], data_range[1], f_range).reshape(f_range, 1).tolist()\n",
        "\n",
        "  new_df = pd.DataFrame.from_dict(new_data)\n",
        "  \n",
        "  for fidx in range(len(features)):\n",
        "    new_df[features[fidx]] = new_df[features[fidx]].map(lambda x: x[0])\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvxcmC2VcIEP"
      },
      "source": [
        "def get_target_values(df, target):\n",
        "  \"\"\"\n",
        "  Return values of target variable based on input feature values.\n",
        "  :param df:               pandas dataframe with input features\n",
        "  :return target_result:   pandas dataframe with input features and target values\n",
        "  \"\"\"\n",
        "  \n",
        "  def calc_target_func(x0, x1, x2, x3):\n",
        "    \"\"\"\n",
        "    Return target value.\n",
        "    \"\"\"\n",
        "\n",
        "    return x0 + (x1**2) + x2 + (2*x3)\n",
        "  \n",
        "\n",
        "  df[target[0]] = df.apply(lambda row: calc_target_func(row['x0'], row['x1'], row['x2'], row['x3']), axis=1)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIpP_7cdc4Sf"
      },
      "source": [
        "def create_extra_train_dataset(df, df_new):\n",
        "  \"\"\"\n",
        "  Extend train dataset with new data.\n",
        "  :param df:               pandas dataframe with input features\n",
        "  :param df_new:           pandas dataframe with input augmented features\n",
        "  :return df_result:       pandas dataframe with input features and target values\n",
        "  \"\"\"\n",
        "\n",
        "  return pd.concat([df, df_new], axis=0).reset_index()[features + target]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3kbER1c4U4"
      },
      "source": [
        "def normalize_features(df, features, target):\n",
        "  \"\"\"\n",
        "  Use Yeo-Johnson transform to normalize input features and target variable.\n",
        "  :param df:        dict with input pandas dataframes\n",
        "  :param features:  list of input features names\n",
        "  :param target:    list of target variables names\n",
        "  :return out_data: dict, data = pandas dataframe with transformed data,\n",
        "                          transformer = transformer objects\n",
        "  \"\"\"\n",
        "  \n",
        "  out_data = {'data': [], 'transformers': []}\n",
        "  transformers = []\n",
        "  df_x = []\n",
        "  df_y = []\n",
        "\n",
        "  # normalize input data\n",
        "  for key in list(df.keys()):\n",
        "\n",
        "    if key == 'train_data':\n",
        "      # fit input features\n",
        "      transformer_f = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "      x_trans = transformer_f.fit_transform(df[key][features])\n",
        "      df_x.append(pd.DataFrame(x_trans, columns=features))\n",
        "      transformers.append(transformer_f)\n",
        "\n",
        "      # fit input target\n",
        "      transformer_t = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "      y_trans = transformer_t.fit_transform(df[key][target])\n",
        "      df_y.append(pd.DataFrame(y_trans, columns=target))\n",
        "      transformers.append(transformer_t)\n",
        "\n",
        "    elif key == 'test_data':\n",
        "      # transform input features\n",
        "      x_trans = transformers[0].transform(df[key][features])\n",
        "      df_x.append(pd.DataFrame(x_trans, columns=features))\n",
        "\n",
        "      # use test target variable\n",
        "      y_trans = transformers[1].transform(df[key][target])\n",
        "      df_y.append(pd.DataFrame(y_trans, columns=target))\n",
        "    \n",
        "    else:\n",
        "      # transform input features\n",
        "      x_trans = transformers[0].transform(df[key][features])\n",
        "      df_x.append(pd.DataFrame(x_trans, columns=features))\n",
        "\n",
        "      # transform target variable\n",
        "      y_trans = transformers[1].transform(df[key][target])\n",
        "      df_y.append(pd.DataFrame(y_trans, columns=target))\n",
        "\n",
        "  # concatenate all transformed features dataframes with transformed target variables\n",
        "  for i in range(len(list(df))):\n",
        "    out_data['data'].append(pd.concat([df_x[i], df_y[i]], axis=1))\n",
        "  \n",
        "  out_data['transformers'] = transformers\n",
        "\n",
        "  return out_data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjIuPZ4xdrnN"
      },
      "source": [
        "def pca(x):\n",
        "  \"\"\"\n",
        "  Get new features by PCA analysis.\n",
        "  :param x:                 pandas dataframe with input features\n",
        "  :return train_pca, pca:   pandas dataframe with transformed data, PCA object\n",
        "  \"\"\"\n",
        "\n",
        "  # Create principal components\n",
        "  pca = PCA()\n",
        "  x_pca = pca.fit_transform(x)\n",
        "\n",
        "  # Convert to dataframe\n",
        "  component_names = [f\"PC{i+1}\" for i in range(x_pca.shape[1])]\n",
        "  train_pca = pd.DataFrame(x_pca, columns=component_names)\n",
        "\n",
        "  return train_pca, pca"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOH6DKO7cIG3"
      },
      "source": [
        "def run_data_prep():\n",
        "  \"\"\"\n",
        "    Prepare data sets.\n",
        "  \"\"\"\n",
        "  df_data = {}\n",
        "  df_extend_data = {}\n",
        "\n",
        "  # split dataset on train, validation and test subsets\n",
        "  target = [columns.pop(-1)]\n",
        "  features = columns\n",
        "  dataset = split_data(df, features, target, train_size, test_size, val_size)\n",
        "\n",
        "  # create train dataset\n",
        "  x_train = dataset['x_train'].reset_index()[features]\n",
        "  y_train = dataset['y_train'].reset_index()[target]\n",
        "  train_data = pd.concat([x_train, y_train], axis=1)\n",
        "  df_data['train_data'] = train_data\n",
        "\n",
        "  # create validation dataset\n",
        "  x_val = dataset['x_val'].reset_index()[features]\n",
        "  y_val = dataset['y_val'].reset_index()[target]\n",
        "  val_data = pd.concat([x_val, y_val], axis=1)\n",
        "  df_data['val_data'] = val_data\n",
        "  \n",
        "  # create test dataset\n",
        "  x_test = dataset['x_test'].reset_index()[features]\n",
        "  y_test = dataset['y_test'].reset_index()[target]\n",
        "  test_data = pd.concat([x_test, y_test], axis=1)\n",
        "  df_data['test_data'] = test_data\n",
        "\n",
        "  # normalize input data\n",
        "  out_data = normalize_features(df_data, features, target)\n",
        "\n",
        "  # create new dataset with more input samples\n",
        "  new_data = cause_relevant_features(dataset['x_train'], features, f_range)\n",
        "  new_data = get_target_values(new_data, target)\n",
        "  new_data = new_data.reset_index()[features + target]\n",
        "  train_new_data = create_extra_train_dataset(train_data, new_data).reset_index()[features + target]\n",
        "  df_extend_data['train_data'] = train_new_data\n",
        "\n",
        "  # normalize input data\n",
        "  out_new_data = normalize_features(df_extend_data, features, target)\n",
        "\n",
        "  return \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}