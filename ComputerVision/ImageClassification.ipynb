{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOU4o9jSnQYnjOKm/L7SVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/DeepLearningProjects/blob/main/ComputerVision/ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V7z6qIHEZrY"
      },
      "source": [
        "# set installation txt file with all relevant packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import pathlib\n",
        "import shutil\n",
        "import random\n",
        "import imghdr\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Rfe6s1Mnty",
        "outputId": "dcdf74a5-72c8-4c9b-e1d8-ea622a175048"
      },
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "  if len(tf.config.list_physical_devices('GPU')) > 1:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "  else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzkCfDG9Qi1Q"
      },
      "source": [
        "# download images folder\n",
        "#!wget https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
        "#from shutil import unpack_archive\n",
        "#unpack_archive('/content/flower_photos.tgz', '/content/')"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN4UgX-jE94e",
        "outputId": "76cc0628-cbe0-4d4a-e594-a5b0093fa391"
      },
      "source": [
        "# load data parameters\n",
        "\n",
        "MAX_WIDTH, MAX_HEIGHT = 100, 100\n",
        "ORIGIN = os.path.dirname(os.path.abspath(\"ImageClassification.ipynb\"))\n",
        "print(ORIGIN)\n",
        "SOURCE = 'flower_photos'\n",
        "#test_path = ''\n",
        "# available color modes are \"grayscale\", \"rgb\", \"rgba\", \n",
        "# (images will be converted to have 1, 3, or 4 channels)\n",
        "COLOR_MODE = \"rgb\"\n",
        "SOURCE_PATH = os.path.join(ORIGIN, SOURCE)\n",
        "#test_source_folder_path = os.path.join(ORIGIN, test_path)\n",
        "print(SOURCE_PATH)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "TEST_SPLIT = 0.2\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 100\n",
        "\n",
        "IMAGE_EXTENSIONS = ['png', 'jpg', 'jpeg', 'tiff', 'bmp', 'gif']\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/flower_photos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGefBXnuLNeo"
      },
      "source": [
        "# split dataset on test and train if is not currently\n",
        "\n",
        "def train_test_split():\n",
        "\n",
        "  train_path = os.path.join(ORIGIN, SOURCE_PATH)\n",
        "  test_path = \"test_data\"\n",
        "  test_path = os.path.join(ORIGIN, test_path)\n",
        "\n",
        "  if os.path.exists(test_path):\n",
        "    shutil.rmtree(test_path)\n",
        "  os.mkdir(test_path)\n",
        "\n",
        "  for dir in os.listdir(train_path):\n",
        "    files = os.listdir(os.path.join(train_path, dir))\n",
        "    if len(files) > 0:\n",
        "      if not os.path.exists(os.path.join(test_path, dir)):\n",
        "        os.mkdir(os.path.join(test_path, dir))\n",
        "      test_num = len(files) * TEST_SPLIT\n",
        "      print(test_num)\n",
        "      indicies = random.sample(range(len(files)), round(test_num))\n",
        "      test_files_names = [files[i] for i in indicies]\n",
        "      # move file from train dataset to test dataset\n",
        "      for f in test_files_names:\n",
        "        image_type = imghdr.what(os.path.join(os.path.join(train_path, dir), f))\n",
        "        #print(image_type)\n",
        "        if image_type in IMAGE_EXTENSIONS:\n",
        "          os.replace(os.path.join(os.path.join(train_path, dir), f), \n",
        "                     os.path.join(os.path.join(test_path, dir), f))\n",
        "\n",
        "  return train_path, test_path\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0072p5n6U8s"
      },
      "source": [
        "def get_min_img_size(train_path, test_path=None):\n",
        "  \"\"\"\n",
        "  Check if all images are of the same size and get min image width and min image\n",
        "  height.\n",
        "  param train_path:              path of a train source folder\n",
        "  param test_path:               path of a test source folder\n",
        "  return img_width, img_height:  min image width and height scalar values\n",
        "  \"\"\"\n",
        "\n",
        "  img_width_list = []\n",
        "  img_height_list = []\n",
        "\n",
        "  def get_image_params(data_path):\n",
        "    img_width, img_height = None, None\n",
        "    dirs = os.listdir(data_path)\n",
        "    for dir in dirs:\n",
        "      files = os.listdir(data_path + \"/\" + dir)\n",
        "      for f in files:\n",
        "        image_path = data_path + \"/\" + dir + \"/\" + f\n",
        "        image = tf.keras.preprocessing.image.load_img(image_path)\n",
        "        image = keras.preprocessing.image.img_to_array(image)\n",
        "        if img_width == None:\n",
        "          img_width, img_height = image.shape[0], image.shape[1]\n",
        "        else:\n",
        "          if image.shape[0] < img_width:\n",
        "            img_width = image.shape[0]\n",
        "          if image.shape[1] < img_height:\n",
        "            img_height = image.shape[1]\n",
        "\n",
        "    return img_width, img_height\n",
        "\n",
        "  if test_path == None:\n",
        "    return get_image_params(train_path)\n",
        "    \n",
        "  else:\n",
        "    img_width_train, img_height_train = get_image_params(train_path)\n",
        "    img_width_list.append(img_width_train)\n",
        "    img_height_list.append(img_height_train)\n",
        "\n",
        "    img_width_test, img_height_test = get_image_params(test_path)\n",
        "    img_width_list.append(img_width_test)\n",
        "    img_height_list.append(img_height_test)\n",
        "  \n",
        "    return min(img_width_list), min(img_height_list)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBaRSrBbrsh"
      },
      "source": [
        "def set_resize_image_parameters(img_width, img_height):\n",
        "  \"\"\"\n",
        "  Set parameters for resizing input images.\n",
        "  param img_width, img_height:   image height and image with scalar values\n",
        "  return img_width, img_height:  resized image height and image with scalar values     \n",
        "  \"\"\"\n",
        "\n",
        "  if img_width > MAX_WIDTH:\n",
        "    img_width = MAX_WIDTH\n",
        "\n",
        "  if img_height > MAX_HEIGHT:\n",
        "    img_height = MAX_HEIGHT\n",
        "  \n",
        "  return img_height, img_width\n",
        "\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ION3btx9esdX",
        "outputId": "9cd490d5-c313-47b0-e238-41a04cadc1a3"
      },
      "source": [
        "# set tensorflow dataset parameters\n",
        "# remove unneccessary files from source directory\n",
        "train_path, test_path = train_test_split()\n",
        "img_width, img_height = get_min_img_size(train_path, test_path=test_path)\n",
        "img_width, img_height = set_resize_image_parameters(img_height, img_width)\n",
        "image_size = (img_width, img_height)\n",
        "print(image_size)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71.60000000000001\n",
            "81.80000000000001\n",
            "64.8\n",
            "65.60000000000001\n",
            "91.80000000000001\n",
            "(100, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2V96MaJRkIt"
      },
      "source": [
        "def get_image_dataset(data_path, image_size, subset = None):\n",
        "  \"\"\"\n",
        "  Create tensorflow dataset from data directory object.\n",
        "  param data_path:        string, source folder path\n",
        "  param image_size:       touple (image_with, image_height)\n",
        "  param subset:           optional, name of subset ('train', 'validation')\n",
        "  return dataset:         dict, tensorflow dataset and class names\n",
        "  \"\"\"\n",
        "    \n",
        "  if subset:\n",
        "    validation_split = VALIDATION_SPLIT\n",
        "    seed = SEED\n",
        "  \n",
        "  else:\n",
        "    validation_split = None\n",
        "    seed = None\n",
        "\n",
        "  dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_path,\n",
        "      color_mode=COLOR_MODE,\n",
        "      image_size=image_size,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      seed=seed,\n",
        "      validation_split=validation_split, \n",
        "      subset=subset\n",
        "    )\n",
        "    \n",
        "  dataset.class_names.sort()\n",
        "    \n",
        "  return {\n",
        "      \"data\": dataset.cache().prefetch(\n",
        "      buffer_size = tf.data.AUTOTUNE\n",
        "      ),\n",
        "      \"classNames\": dataset.class_names\n",
        "        }"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE9YzOR-LSdZ",
        "outputId": "e88e7dc1-cd1c-461c-a40d-6d7cb373512f"
      },
      "source": [
        "\n",
        "training_ds = get_image_dataset(\n",
        "    train_path,\n",
        "    image_size,\n",
        "    subset = \"training\"\n",
        ")\n",
        "\n",
        "validation_ds = get_image_dataset(\n",
        "    train_path,\n",
        "    image_size,\n",
        "    subset = \"validation\"\n",
        ")\n",
        "\n",
        "\n",
        "test_ds = get_image_dataset(\n",
        "    test_path,\n",
        "    image_size\n",
        ")\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1501 files belonging to 6 classes.\n",
            "Using 1201 files for training.\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op DummyMemoryCache in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op CacheDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Found 1501 files belonging to 6 classes.\n",
            "Using 300 files for validation.\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op DummyMemoryCache in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op CacheDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Found 377 files belonging to 5 classes.\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op DummySeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV3 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op DummyMemoryCache in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op CacheDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3jMS-Co12sM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}